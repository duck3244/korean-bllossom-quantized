# ğŸŒ¸ Korean Bllossom AICA-5B ì–‘ìí™” í”„ë¡œì íŠ¸

RTX 4060 8GBì—ì„œ ìµœì í™”ëœ í•œêµ­ì–´-ì˜ì–´ ì‹œê°-ì–¸ì–´ ëª¨ë¸ ì‹¤í–‰ í™˜ê²½

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![CUDA](https://img.shields.io/badge/CUDA-11.8+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” [Bllossom/llama-3.2-Korean-Bllossom-AICA-5B](https://huggingface.co/Bllossom/llama-3.2-Korean-Bllossom-AICA-5B) ëª¨ë¸ì„ RTX 4060 8GB í™˜ê²½ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ 4-bit ì–‘ìí™” ê¸°ìˆ ì„ ì ìš©í•œ ìµœì í™” ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.

### âœ¨ ì£¼ìš” íŠ¹ì§•

- **ğŸ¯ RTX 4060 8GB ìµœì í™”**: 4-bit NF4 ì–‘ìí™”ë¡œ VRAM ì‚¬ìš©ëŸ‰ 75% ì ˆì•½
- **ğŸ”„ ì´ì¤‘ ëª¨ë“œ ì§€ì›**: í…ìŠ¤íŠ¸ ìƒì„± + ì‹œê°-ì–¸ì–´ ëª¨ë¸
- **ğŸŒ ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´/ì˜ì–´ ì™„ì „ ì§€ì›
- **âš¡ ê³ ì„±ëŠ¥**: 15-25 í† í°/ì´ˆ ìƒì„± ì†ë„
- **ğŸ› ï¸ ëª¨ë“ˆí™” ì„¤ê³„**: ê¸°ëŠ¥ë³„ ë…ë¦½ ëª¨ë“ˆ
- **ğŸ“Š ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 

### ğŸ® ì§€ì› í•˜ë“œì›¨ì–´

| GPU ëª¨ë¸ | VRAM | ì§€ì› ì—¬ë¶€ | ì„±ëŠ¥ |
|----------|------|-----------|------|
| RTX 4060 | 8GB | âœ… ìµœì í™” | ìš°ìˆ˜ |
| RTX 4060 Ti | 16GB | âœ… ì™„ë²½ | ìµœê³  |
| RTX 4070 | 12GB | âœ… ì™„ë²½ | ìµœê³  |
| RTX 3060 | 12GB | âœ… ì–‘í˜¸ | ì¢‹ìŒ |
| RTX 3060 Ti | 8GB | âš ï¸ ì œí•œì  | ë³´í†µ |

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ìš”êµ¬ì‚¬í•­ í™•ì¸

```bash
# GPU í™•ì¸
nvidia-smi

# Python ë²„ì „ í™•ì¸ (3.8+ í•„ìš”)
python3 --version

# ì—¬ìœ  ê³µê°„ í™•ì¸ (50GB+ ê¶Œì¥)
df -h
```

### 2. í”„ë¡œì íŠ¸ ì„¤ì¹˜

```bash
# í”„ë¡œì íŠ¸ í´ë¡ 
git clone <repository-url>
cd korean-bllossom-quantized

# ìë™ ì„¤ì¹˜ ì‹¤í–‰
chmod +x setup.sh
./setup.sh
```

### 3. ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source bllossom_env/bin/activate

# ì‹œìŠ¤í…œ í™•ì¸
python main.py --check

# ë°ëª¨ ì‹¤í–‰
python main.py --demo
```

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
korean-bllossom-quantized/
â”œâ”€â”€ ğŸ“„ core/                    # í•µì‹¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ config.py              # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ model_manager.py       # ëª¨ë¸ ë¡œë”©/ê´€ë¦¬
â”‚   â”œâ”€â”€ text_generator.py      # í…ìŠ¤íŠ¸ ìƒì„±
â”‚   â””â”€â”€ vision_generator.py    # ì‹œê°-ì–¸ì–´ ìƒì„±
â”œâ”€â”€ ğŸ–¥ï¸ interfaces/             # ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ cli_interface.py       # ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤
â”œâ”€â”€ ğŸ“‹ scripts/               # ìœ í‹¸ë¦¬í‹° ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ setup.sh              # ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“š docs/                  # ë¬¸ì„œ
â”‚   â”œâ”€â”€ INSTALL.md           # ì„¤ì¹˜ ê°€ì´ë“œ
â”‚   â”œâ”€â”€ USAGE.md             # ì‚¬ìš©ë²• ê°€ì´ë“œ
â”‚   â””â”€â”€ API.md               # API ë¬¸ì„œ
â”œâ”€â”€ ğŸ—‚ï¸ data/                 # ë°ì´í„° ë””ë ‰í† ë¦¬
â”œâ”€â”€ ğŸ’¾ model_cache/          # ëª¨ë¸ ìºì‹œ
â”œâ”€â”€ ğŸ“Š logs/                 # ë¡œê·¸ íŒŒì¼
â”œâ”€â”€ ğŸ“¤ outputs/              # ì¶œë ¥ íŒŒì¼
â”œâ”€â”€ main.py                   # ë©”ì¸ ì‹¤í–‰ íŒŒì¼
â”œâ”€â”€ requirements.txt          # ì˜ì¡´ì„± ëª©ë¡
â””â”€â”€ config.yaml              # ì„¤ì • íŒŒì¼
```

## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥

### ğŸ¤– í…ìŠ¤íŠ¸ ìƒì„±

```python
from text_generator import TextGenerator
from model_manager import get_model_manager
from config import Config

# ëª¨ë¸ ì´ˆê¸°í™”
config = Config()
manager = get_model_manager(config)
manager.load_model()

# í…ìŠ¤íŠ¸ ìƒì„±
generator = TextGenerator(manager, config)
result = generator.generate("ì•ˆë…•í•˜ì„¸ìš”! AIì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
print(result['response'])
```

### ğŸ–¼ï¸ ì‹œê°-ì–¸ì–´ ëª¨ë¸

```python
from vision_generator import VisionGenerator

# ì´ë¯¸ì§€ ë¶„ì„
vision_gen = VisionGenerator(manager, config)
result = vision_gen.describe_image("image.jpg")
print(result['response'])

# OCR (í…ìŠ¤íŠ¸ ì¶”ì¶œ)
ocr_result = vision_gen.extract_text("document.png")
print(ocr_result['response'])
```

### ğŸ’¬ ëŒ€í™”í˜• ì±„íŒ…

```python
from text_generator import ConversationManager

# ëŒ€í™” ê´€ë¦¬ì ì´ˆê¸°í™”
conversation = ConversationManager(generator)
conversation.set_system_message("ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")

# ëŒ€í™”
response = conversation.generate_response("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?")
print(response['response'])
```

## ğŸ”§ CLI ì‚¬ìš©ë²•

### í…ìŠ¤íŠ¸ ìƒì„±

```bash
# ë‹¨ì¼ í…ìŠ¤íŠ¸ ìƒì„±
python cli_interface.py text "í•œêµ­ì˜ ì—­ì‚¬ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”"

# ëŒ€í™”í˜• ëª¨ë“œ
python cli_interface.py text --interactive

# íŒŒë¼ë¯¸í„° ì¡°ì •
python cli_interface.py text "ì°½ì‘ ì†Œì„¤ì„ ì¨ì£¼ì„¸ìš”" \
  --max-tokens 500 --temperature 0.9
```

### ì´ë¯¸ì§€ ë¶„ì„

```bash
# ì´ë¯¸ì§€ ì„¤ëª…
python cli_interface.py vision image.jpg --task describe

# OCR (í…ìŠ¤íŠ¸ ì¶”ì¶œ)
python cli_interface.py vision document.png --task ocr

# ì°¨íŠ¸ ë¶„ì„
python cli_interface.py vision chart.png --task chart

# ì‚¬ìš©ì ì •ì˜ ì§ˆë¬¸
python cli_interface.py vision photo.jpg --task qa \
  --prompt "ì´ ì‚¬ì§„ì—ì„œ ì‚¬ëŒì´ ëª‡ ëª…ì¸ê°€ìš”?"
```

### ë¬¸ì„œ ì²˜ë¦¬

```bash
# ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬
python cli_interface.py document doc.png --task markdown

# ë‹¤ì¤‘ í˜ì´ì§€ ë¬¸ì„œ
python cli_interface.py document docs_folder/ \
  --task extract --multi-page

# í‘œ ë°ì´í„° ì¶”ì¶œ
python cli_interface.py document table.png --task table
```

### ë°°ì¹˜ ì²˜ë¦¬

```bash
# í…ìŠ¤íŠ¸ ë°°ì¹˜ ì²˜ë¦¬
python cli_interface.py batch prompts.txt --output results.json

# ì´ë¯¸ì§€ ë°°ì¹˜ ë¶„ì„
python cli_interface.py batch image_tasks.json --output analysis.json
```

## âš™ï¸ ì„¤ì • ê´€ë¦¬

### config.yaml êµ¬ì¡°

```yaml
model:
  name: "Bllossom/llama-3.2-Korean-Bllossom-AICA-5B"
  trust_remote_code: true
  torch_dtype: "bfloat16"

quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"

generation:
  max_tokens: 256
  temperature: 0.7
  top_p: 0.9
  top_k: 50

hardware:
  target_gpu: "RTX 4060"
  target_vram_gb: 8
  max_memory_usage: 0.9
```

### ì„±ëŠ¥ ìµœì í™” ì„¤ì •

```python
# ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ
config.quantization.load_in_4bit = True
config.generation.max_tokens = 200

# ê³ í’ˆì§ˆ ëª¨ë“œ (ë” ë§ì€ VRAM í•„ìš”)
config.quantization.load_in_4bit = False
config.model.torch_dtype = "float16"

# ì†ë„ ìš°ì„  ëª¨ë“œ
config.generation.temperature = 0.1
config.generation.do_sample = False
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### RTX 4060 8GB í…ŒìŠ¤íŠ¸ ê²°ê³¼

| ì‘ì—… | VRAM ì‚¬ìš©ëŸ‰ | ìƒì„± ì†ë„ | í’ˆì§ˆ ì ìˆ˜ |
|------|-------------|-----------|-----------|
| í…ìŠ¤íŠ¸ ìƒì„± | 5.2GB | 22.3 t/s | 8.5/10 |
| ì´ë¯¸ì§€ ì„¤ëª… | 6.8GB | 18.7 t/s | 8.2/10 |
| OCR | 6.5GB | 20.1 t/s | 9.1/10 |
| ë¬¸ì„œ ë³€í™˜ | 6.9GB | 17.5 t/s | 8.8/10 |

### ë‹¤ë¥¸ GPUì™€ì˜ ë¹„êµ

| GPU | VRAM | ì†ë„ ë°°ìˆ˜ | ìµœëŒ€ í† í° |
|-----|------|-----------|-----------|
| RTX 4060 8GB | 8GB | 1.0x | 4096 |
| RTX 4060 Ti 16GB | 16GB | 1.3x | 8192 |
| RTX 4070 12GB | 12GB | 1.2x | 6144 |
| RTX 3060 12GB | 12GB | 0.8x | 6144 |

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

#### VRAM ë¶€ì¡± ì˜¤ë¥˜

```bash
# í•´ê²° ë°©ë²• 1: ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
nvidia-smi
sudo kill -9 <PID>

# í•´ê²° ë°©ë²• 2: ìºì‹œ ì •ë¦¬
rm -rf ~/.cache/huggingface/
rm -rf ./model_cache/

# í•´ê²° ë°©ë²• 3: ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©
python main.py --chat  # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰
```

#### ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨

```bash
# ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ í•´ê²°
export HF_HUB_DISABLE_SYMLINKS_WARNING=1
export HF_HUB_CACHE=./model_cache

# ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python -c "
from transformers import MllamaProcessor
processor = MllamaProcessor.from_pretrained(
    'Bllossom/llama-3.2-Korean-Bllossom-AICA-5B',
    cache_dir='./model_cache'
)
"
```

#### ì˜ì¡´ì„± ì¶©ëŒ

```bash
# ê°€ìƒí™˜ê²½ ì¬ìƒì„±
rm -rf bllossom_env
python3 -m venv bllossom_env
source bllossom_env/bin/activate
pip install -r requirements.txt
```

### ì„±ëŠ¥ ìµœì í™” íŒ

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°

```python
# 1. ë” ì‘ì€ ì •ë°€ë„ ì‚¬ìš©
config.quantization.bnb_4bit_compute_dtype = "float16"

# 2. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
max_tokens = 128  # ê¸°ë³¸ê°’ 256ì—ì„œ ì¤„ì„

# 3. ìºì‹œ ë¹„í™œì„±í™”
use_cache = False
```

#### ìƒì„± ì†ë„ ë†’ì´ê¸°

```python
# 1. ìƒ˜í”Œë§ ë¹„í™œì„±í™”
do_sample = False
temperature = 0.0

# 2. ë¹” ì„œì¹˜ ì‚¬ìš©
num_beams = 1

# 3. ì¡°ê¸° ì¢…ë£Œ í™œì„±í™”
early_stopping = True
```

## ğŸ“š ì‚¬ìš© ì‚¬ë¡€

### 1. ê°œì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸

```python
# ì¼ìƒ ëŒ€í™” ë° ì§ˆë¬¸ë‹µë³€
conversation.set_system_message(
    "ë‹¹ì‹ ì€ í•œêµ­ì–´ë¥¼ ì˜í•˜ëŠ” ì¹œê·¼í•œ ê°œì¸ ë¹„ì„œì…ë‹ˆë‹¤."
)

response = conversation.generate_response(
    "ì˜¤ëŠ˜ í•  ì¼ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”."
)
```

### 2. ë¬¸ì„œ ì²˜ë¦¬ ìë™í™”

```python
# ëŒ€ëŸ‰ ë¬¸ì„œ OCR ì²˜ë¦¬
from vision_generator import DocumentProcessor

processor = DocumentProcessor(vision_generator)
results = processor.process_multi_page_document(
    ["page1.png", "page2.png", "page3.png"],
    task="markdown"
)
```

### 3. êµìœ¡ ì½˜í…ì¸  ìƒì„±

```python
# ë§ì¶¤í˜• í•™ìŠµ ìë£Œ ìƒì„±
result = text_generator.generate(
    "ì¤‘í•™ìƒì„ ìœ„í•œ ê´‘í•©ì„± ì„¤ëª…ì„ ì‰½ê²Œ ì¨ì£¼ì„¸ìš”.",
    max_tokens=400,
    temperature=0.6
)
```

### 4. ì°½ì‘ ì§€ì› ë„êµ¬

```python
# ì†Œì„¤/ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„± ì§€ì›
result = text_generator.generate(
    "íŒíƒ€ì§€ ì†Œì„¤ì˜ í¥ë¯¸ì§„ì§„í•œ ëª¨í—˜ ì¥ë©´ì„ ì¨ì£¼ì„¸ìš”.",
    max_tokens=500,
    temperature=0.9
)
```

## ğŸ”¬ ê³ ê¸‰ ê¸°ëŠ¥

### íŒŒì¸íŠœë‹ (ì‹¤í—˜ì )

```python
# LoRA íŒŒì¸íŠœë‹ ì¤€ë¹„
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1
)

# íŒŒì¸íŠœë‹ ì‹¤í–‰ (ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ í•„ìš”)
```

### ëª¨ë¸ ë³€í™˜

```python
# GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (llama.cpp í˜¸í™˜)
python scripts/model_converter.py \
  --input Bllossom/llama-3.2-Korean-Bllossom-AICA-5B \
  --output ./models/bllossom-q4.gguf \
  --quantization q4_0
```

### ë¶„ì‚° ì¶”ë¡ 

```python
# ë‹¤ì¤‘ GPU ì„¤ì • (2ê°œ ì´ìƒì˜ GPU í•„ìš”)
config.model.device_map = {
    "model.embed_tokens": 0,
    "model.layers.0-15": 0,
    "model.layers.16-31": 1,
    "model.norm": 1,
    "lm_head": 1
}
```

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements-dev.txt

# ì½”ë“œ ìŠ¤íƒ€ì¼ í™•ì¸
black --check .
flake8 .

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/
```
