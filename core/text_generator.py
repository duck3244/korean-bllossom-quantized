# text_generator.py
# í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥

import torch
import time
from typing import List, Dict, Optional, Any
from core.model_manager import ModelManager
from core.config import Config
import logging

logger = logging.getLogger(__name__)

class TextGenerator:
    """í…ìŠ¤íŠ¸ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self, model_manager: ModelManager, config: Config):
        self.model_manager = model_manager
        self.config = config
    
    def generate(
        self, 
        prompt: str, 
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        repetition_penalty: Optional[float] = None,
        do_sample: Optional[bool] = None,
        system_message: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„ (0.0-2.0)
            top_p: nucleus sampling íŒŒë¼ë¯¸í„°
            top_k: top-k sampling íŒŒë¼ë¯¸í„°
            repetition_penalty: ë°˜ë³µ ì–µì œ ì •ë„
            do_sample: ìƒ˜í”Œë§ ì—¬ë¶€
            system_message: ì‹œìŠ¤í…œ ë©”ì‹œì§€
            
        Returns:
            ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        if not self.model_manager.is_loaded:
            return {
                "success": False,
                "error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "response": None
            }
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        max_tokens = max_tokens or self.config.generation.max_tokens
        temperature = temperature or self.config.generation.temperature
        top_p = top_p or self.config.generation.top_p
        top_k = top_k or self.config.generation.top_k
        repetition_penalty = repetition_penalty or self.config.generation.repetition_penalty
        do_sample = do_sample if do_sample is not None else self.config.generation.do_sample
        
        print(f"ğŸ’¬ ì…ë ¥: {prompt}")
        print("ğŸ¤– ìƒì„± ì¤‘...")
        
        start_time = time.time()
        
        try:
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = []
            
            if system_message:
                messages.append({
                    'role': 'system', 
                    'content': [{'type': 'text', 'text': system_message}]
                })
            
            messages.append({
                'role': 'user', 
                'content': [{'type': 'text', 'text': prompt}]
            })
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            input_text = self.model_manager.processor.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.model_manager.processor(
                images=None,  # í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë“œ
                text=input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to(self.model_manager.device)
            
            # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
            generation_kwargs = {
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "do_sample": do_sample,
                "pad_token_id": self.model_manager.processor.tokenizer.eos_token_id,
                "eos_token_id": self.model_manager.processor.tokenizer.convert_tokens_to_ids('<|eot_id|>'),
                "use_cache": True
            }
            
            if do_sample:
                generation_kwargs.update({
                    "top_p": top_p,
                    "top_k": top_k,
                    "repetition_penalty": repetition_penalty
                })
            
            # í…ìŠ¤íŠ¸ ìƒì„±
            with torch.no_grad():
                output = self.model_manager.model.generate(
                    **inputs,
                    **generation_kwargs
                )
            
            # ë””ì½”ë”©
            full_response = self.model_manager.processor.decode(output[0], skip_special_tokens=True)
            
            # ì‘ë‹µ ì¶”ì¶œ
            clean_response = self._extract_response(full_response, input_text)
            
            generation_time = time.time() - start_time
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(output[0]) - input_tokens
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            print(f"âœ¨ ì‘ë‹µ: {clean_response}")
            print(f"â±ï¸ ìƒì„± ì‹œê°„: {generation_time:.2f}ì´ˆ")
            print(f"ğŸš€ ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ")
            
            return {
                "success": True,
                "response": clean_response,
                "full_response": full_response,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_second": tokens_per_second,
                "parameters": {
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k,
                    "repetition_penalty": repetition_penalty,
                    "do_sample": do_sample
                }
            }
            
        except torch.cuda.OutOfMemoryError:
            self.model_manager.clear_memory()
            error_msg = "VRAM ë¶€ì¡±! ë” ì§§ì€ í”„ë¡¬í”„íŠ¸ë‚˜ ì ì€ í† í° ìˆ˜ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”."
            print(f"âŒ {error_msg}")
            return {
                "success": False,
                "error": error_msg,
                "response": None
            }
            
        except Exception as e:
            error_msg = f"ìƒì„± ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            logger.error(f"Text generation failed: {e}")
            return {
                "success": False,
                "error": error_msg,
                "response": None
            }
    
    def _extract_response(self, full_response: str, input_text: str) -> str:
        """ì‘ë‹µì—ì„œ ì‹¤ì œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        try:
            # assistant íƒœê·¸ ì´í›„ì˜ ë‚´ìš© ì¶”ì¶œ
            if "assistant\n\n" in full_response:
                response_start = full_response.find("assistant\n\n") + len("assistant\n\n")
                clean_response = full_response[response_start:].strip()
            elif "assistant" in full_response:
                clean_response = full_response.split("assistant")[-1].strip()
            else:
                # ì…ë ¥ ë¶€ë¶„ ì œê±°
                clean_response = full_response.replace(input_text, "").strip()
            
            # íŠ¹ìˆ˜ í† í° ì •ë¦¬
            clean_response = clean_response.replace('<|eot_id|>', '').strip()
            
            return clean_response
            
        except Exception as e:
            logger.warning(f"Response extraction failed: {e}")
            return full_response
    
    def chat_generate(self, messages: List[Dict], **kwargs) -> Dict[str, Any]:
        """
        ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            messages: ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user/assistant", "content": "..."}]
            **kwargs: ìƒì„± íŒŒë¼ë¯¸í„°
            
        Returns:
            ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        
        if not self.model_manager.is_loaded:
            return {
                "success": False,
                "error": "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "response": None
            }
        
        try:
            # ë©”ì‹œì§€ë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
            formatted_messages = []
            for msg in messages:
                if isinstance(msg["content"], str):
                    content = [{"type": "text", "text": msg["content"]}]
                else:
                    content = msg["content"]
                
                formatted_messages.append({
                    "role": msg["role"],
                    "content": content
                })
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            input_text = self.model_manager.processor.apply_chat_template(
                formatted_messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # í† í¬ë‚˜ì´ì§•
            inputs = self.model_manager.processor(
                images=None,
                text=input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to(self.model_manager.device)
            
            # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
            generation_kwargs = {
                "max_new_tokens": kwargs.get("max_tokens", self.config.generation.max_tokens),
                "temperature": kwargs.get("temperature", self.config.generation.temperature),
                "do_sample": kwargs.get("do_sample", self.config.generation.do_sample),
                "pad_token_id": self.model_manager.processor.tokenizer.eos_token_id,
                "eos_token_id": self.model_manager.processor.tokenizer.convert_tokens_to_ids('<|eot_id|>'),
                "use_cache": True
            }
            
            if generation_kwargs["do_sample"]:
                generation_kwargs.update({
                    "top_p": kwargs.get("top_p", self.config.generation.top_p),
                    "top_k": kwargs.get("top_k", self.config.generation.top_k),
                    "repetition_penalty": kwargs.get("repetition_penalty", self.config.generation.repetition_penalty)
                })
            
            start_time = time.time()
            
            # ìƒì„±
            with torch.no_grad():
                output = self.model_manager.model.generate(
                    **inputs,
                    **generation_kwargs
                )
            
            # ê²°ê³¼ ì²˜ë¦¬
            full_response = self.model_manager.processor.decode(output[0], skip_special_tokens=True)
            clean_response = self._extract_response(full_response, input_text)
            
            generation_time = time.time() - start_time
            input_tokens = len(inputs['input_ids'][0])
            output_tokens = len(output[0]) - input_tokens
            tokens_per_second = output_tokens / generation_time if generation_time > 0 else 0
            
            return {
                "success": True,
                "response": clean_response,
                "full_response": full_response,
                "generation_time": generation_time,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "tokens_per_second": tokens_per_second,
                "parameters": generation_kwargs
            }
            
        except Exception as e:
            error_msg = f"ëŒ€í™” ìƒì„± ì‹¤íŒ¨: {str(e)}"
            logger.error(error_msg)
            return {
                "success": False,
                "error": error_msg,
                "response": None
            }
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompts: í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
            **kwargs: ìƒì„± íŒŒë¼ë¯¸í„°
            
        Returns:
            ìƒì„± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        print(f"ğŸ“¦ ë°°ì¹˜ ìƒì„± ì‹œì‘: {len(prompts)}ê°œ í”„ë¡¬í”„íŠ¸")
        
        for i, prompt in enumerate(prompts):
            print(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {i+1}/{len(prompts)}")
            result = self.generate(prompt, **kwargs)
            results.append(result)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            if i % 5 == 4:  # 5ê°œë§ˆë‹¤ ë©”ëª¨ë¦¬ ì •ë¦¬
                self.model_manager.clear_memory()
        
        print("âœ… ë°°ì¹˜ ìƒì„± ì™„ë£Œ!")
        return results
    
    def stream_generate(self, prompt: str, **kwargs):
        """
        ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            **kwargs: ìƒì„± íŒŒë¼ë¯¸í„°
            
        Yields:
            ìƒì„±ëœ í† í°ë“¤
        """
        # TODO: ìŠ¤íŠ¸ë¦¬ë° ìƒì„± êµ¬í˜„
        # í˜„ì¬ëŠ” ì¼ë°˜ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´
        result = self.generate(prompt, **kwargs)
        if result["success"]:
            yield result["response"]
        else:
            yield f"ì˜¤ë¥˜: {result['error']}"


class ConversationManager:
    """ëŒ€í™” ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, text_generator: TextGenerator, max_history: int = 10):
        self.text_generator = text_generator
        self.max_history = max_history
        self.conversation_history = []
        self.system_message = None
    
    def set_system_message(self, message: str):
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •"""
        self.system_message = message
    
    def add_message(self, role: str, content: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ë©”ì‹œì§€ ì¶”ê°€"""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": time.time()
        })
        
        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ì œí•œ
        if len(self.conversation_history) > self.max_history * 2:  # user + assistant ìŒ
            self.conversation_history = self.conversation_history[-self.max_history * 2:]
    
    def generate_response(self, user_input: str, **kwargs) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.add_message("user", user_input)
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        messages = []
        
        if self.system_message:
            messages.append({
                "role": "system",
                "content": self.system_message
            })
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        for msg in self.conversation_history:
            messages.append({
                "role": msg["role"],
                "content": msg["content"]
            })
        
        # ì‘ë‹µ ìƒì„±
        result = self.text_generator.chat_generate(messages, **kwargs)
        
        if result["success"]:
            # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€
            self.add_message("assistant", result["response"])
        
        return result
    
    def clear_history(self):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.conversation_history = []
        print("ğŸ—‘ï¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_history_summary(self) -> Dict[str, Any]:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½ ë°˜í™˜"""
        return {
            "total_messages": len(self.conversation_history),
            "user_messages": len([msg for msg in self.conversation_history if msg["role"] == "user"]),
            "assistant_messages": len([msg for msg in self.conversation_history if msg["role"] == "assistant"]),
            "system_message": self.system_message,
            "latest_messages": self.conversation_history[-4:] if self.conversation_history else []
        }
    
    def export_conversation(self, filename: str = None) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        if filename is None:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f"conversation_{timestamp}.txt"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("=== Korean Bllossom ëŒ€í™” ê¸°ë¡ ===\n\n")
                
                if self.system_message:
                    f.write(f"ì‹œìŠ¤í…œ ë©”ì‹œì§€: {self.system_message}\n\n")
                
                for msg in self.conversation_history:
                    timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(msg["timestamp"]))
                    role_name = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
                    f.write(f"[{timestamp}] {role_name}: {msg['content']}\n\n")
            
            print(f"ğŸ“ ëŒ€í™”ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filename}")
            return filename
            
        except Exception as e:
            print(f"âŒ ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: {e}")
            return None


if __name__ == "__main__":
    # í…ìŠ¤íŠ¸ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸
    from config import config
    from model_manager import get_model_manager
    
    # ëª¨ë¸ ë§¤ë‹ˆì € ì´ˆê¸°í™”
    manager = get_model_manager(config)
    
    if manager.load_model():
        # í…ìŠ¤íŠ¸ ìƒì„±ê¸° ì´ˆê¸°í™”
        generator = TextGenerator(manager, config)
        
        # ë‹¨ì¼ ìƒì„± í…ŒìŠ¤íŠ¸
        result = generator.generate("ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?")
        print(f"ìƒì„± ê²°ê³¼: {result}")
        
        # ëŒ€í™” ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
        conversation = ConversationManager(generator)
        conversation.set_system_message("ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
        
        # ëŒ€í™” í…ŒìŠ¤íŠ¸
        response1 = conversation.generate_response("ì•ˆë…•í•˜ì„¸ìš”!")
        print(f"ì‘ë‹µ 1: {response1}")
        
        response2 = conversation.generate_response("ì˜¤ëŠ˜ ë­ í•˜ê³  ê³„ì„¸ìš”?")
        print(f"ì‘ë‹µ 2: {response2}")
        
        # íˆìŠ¤í† ë¦¬ ìš”ì•½
        summary = conversation.get_history_summary()
        print(f"ëŒ€í™” ìš”ì•½: {summary}")
        
        # ëŒ€í™” ì €ì¥
        conversation.export_conversation()
        
        manager.unload_model()
    else:
        print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")