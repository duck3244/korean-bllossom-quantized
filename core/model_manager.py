# model_manager.py
# ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬

import torch
import gc
import psutil
from transformers import (
    MllamaForConditionalGeneration, 
    MllamaProcessor,
    BitsAndBytesConfig
)
from core.config import Config
import logging
import time
from typing import Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelManager:
    """ëª¨ë¸ ë¡œë”© ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Config):
        self.config = config
        self.model = None
        self.processor = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.is_loaded = False
        
        # ì‹œìŠ¤í…œ ì •ë³´ í™•ì¸
        self._check_system_requirements()
    
    def _check_system_requirements(self):
        """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸"""
        print("ğŸ” ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸ ì¤‘...")
        
        # CPU ì •ë³´
        cpu_count = psutil.cpu_count()
        ram_gb = psutil.virtual_memory().total / (1024**3)
        print(f"ğŸ’» CPU: {cpu_count}ì½”ì–´")
        print(f"ğŸ§  RAM: {ram_gb:.1f}GB")
        
        # GPU ì •ë³´
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name()
            total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"ğŸ® GPU: {gpu_name}")
            print(f"ğŸ’¾ VRAM: {total_vram:.1f}GB")
            
            # ê¶Œì¥ì‚¬í•­ í™•ì¸
            if total_vram < 6:
                print("âš ï¸ ê²½ê³ : VRAMì´ 6GB ë¯¸ë§Œì…ë‹ˆë‹¤. ì„±ëŠ¥ ì €í•˜ê°€ ì˜ˆìƒë©ë‹ˆë‹¤.")
            elif total_vram >= 8:
                print("âœ… VRAMì´ ì¶©ë¶„í•©ë‹ˆë‹¤.")
        else:
            print("âŒ CUDA GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤. (ë§¤ìš° ëŠë¦¼)")
    
    def _setup_quantization_config(self) -> BitsAndBytesConfig:
        """ì–‘ìí™” ì„¤ì • ìƒì„±"""
        return BitsAndBytesConfig(
            load_in_4bit=self.config.quantization.load_in_4bit,
            bnb_4bit_quant_type=self.config.quantization.bnb_4bit_quant_type,
            bnb_4bit_compute_dtype=getattr(torch, self.config.quantization.bnb_4bit_compute_dtype),
            bnb_4bit_use_double_quant=self.config.quantization.bnb_4bit_use_double_quant,
        )
    
    def load_model(self) -> bool:
        """ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ"""
        if self.is_loaded:
            print("âœ… ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return True
        
        print("ğŸš€ Korean Bllossom AICA-5B ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        start_time = time.time()
        
        try:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            self.clear_memory()
            
            # ì–‘ìí™” ì„¤ì •
            quantization_config = self._setup_quantization_config()
            
            # ëª¨ë¸ ë¡œë“œ
            print("ğŸ“¦ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì¤‘...")
            self.model = MllamaForConditionalGeneration.from_pretrained(
                self.config.model.name,
                quantization_config=quantization_config,
                device_map=self.config.model.device_map,
                torch_dtype=getattr(torch, self.config.model.torch_dtype),
                low_cpu_mem_usage=self.config.model.low_cpu_mem_usage,
                trust_remote_code=self.config.model.trust_remote_code,
                cache_dir=self.config.paths.cache_dir
            )
            
            # í”„ë¡œì„¸ì„œ ë¡œë“œ
            print("ğŸ”§ í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...")
            self.processor = MllamaProcessor.from_pretrained(
                self.config.model.name,
                trust_remote_code=self.config.model.trust_remote_code,
                cache_dir=self.config.paths.cache_dir
            )
            
            self.is_loaded = True
            load_time = time.time() - start_time
            
            print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ({load_time:.1f}ì´ˆ)")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            self._print_memory_usage()
            
            return True
            
        except torch.cuda.OutOfMemoryError:
            print("âŒ VRAM ë¶€ì¡±ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨!")
            print("ğŸ’¡ í•´ê²°ì±…:")
            print("1. ë‹¤ë¥¸ GPU ì‚¬ìš© í”„ë¡œê·¸ë¨ë“¤ì„ ì¢…ë£Œí•˜ì„¸ìš”")
            print("2. ì‹œìŠ¤í…œì„ ì¬ë¶€íŒ…í•˜ì„¸ìš”")
            print("3. ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”")
            self.clear_memory()
            return False
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            logger.error(f"Model loading failed: {e}")
            return False
    
    def unload_model(self):
        """ëª¨ë¸ ì–¸ë¡œë“œ"""
        if not self.is_loaded:
            print("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ìƒíƒœì…ë‹ˆë‹¤.")
            return
        
        print("ğŸ“¤ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘...")
        
        try:
            if self.model is not None:
                del self.model
                self.model = None
            
            if self.processor is not None:
                del self.processor
                self.processor = None
            
            self.is_loaded = False
            self.clear_memory()
            
            print("âœ… ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì–¸ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def clear_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def _print_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            reserved = torch.cuda.memory_reserved(0) / (1024**3)
            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            print(f"ğŸ“Š VRAM ì‚¬ìš©ëŸ‰:")
            print(f"   í• ë‹¹ë¨: {allocated:.1f}GB")
            print(f"   ì˜ˆì•½ë¨: {reserved:.1f}GB")
            print(f"   ì „ì²´: {total:.1f}GB")
            print(f"   ì‚¬ìš©ë¥ : {(allocated/total)*100:.1f}%")
            
            # ê²½ê³  í™•ì¸
            usage_ratio = allocated / total
            if usage_ratio > 0.9:
                print("âš ï¸ ê²½ê³ : VRAM ì‚¬ìš©ë¥ ì´ 90%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
            elif usage_ratio > 0.8:
                print("âš ï¸ ì£¼ì˜: VRAM ì‚¬ìš©ë¥ ì´ 80%ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")
        
        # ì‹œìŠ¤í…œ RAM ì‚¬ìš©ëŸ‰
        ram = psutil.virtual_memory()
        ram_used_gb = ram.used / (1024**3)
        ram_total_gb = ram.total / (1024**3)
        print(f"ğŸ§  RAM ì‚¬ìš©ëŸ‰: {ram_used_gb:.1f}GB / {ram_total_gb:.1f}GB ({ram.percent:.1f}%)")
    
    def get_model_info(self) -> dict:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        if not self.is_loaded:
            return {"status": "not_loaded"}
        
        info = {
            "status": "loaded",
            "model_name": self.config.model.name,
            "device": self.device,
            "quantization": self.config.quantization.bnb_4bit_quant_type,
            "torch_dtype": self.config.model.torch_dtype
        }
        
        if torch.cuda.is_available():
            info["vram_allocated"] = torch.cuda.memory_allocated(0) / (1024**3)
            info["vram_total"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        return info
    
    def health_check(self) -> bool:
        """ëª¨ë¸ ìƒíƒœ í™•ì¸"""
        if not self.is_loaded:
            return False
        
        try:
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„±
            test_messages = [
                {'role': 'user', 'content': [
                    {'type': 'text', 'text': 'ì•ˆë…•í•˜ì„¸ìš”'}
                ]},
            ]
            
            input_text = self.processor.apply_chat_template(
                test_messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            inputs = self.processor(
                images=None,
                text=input_text,
                add_special_tokens=False,
                return_tensors="pt",
            ).to(self.device)
            
            # ë§¤ìš° ì§§ì€ ìƒì„±ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            with torch.no_grad():
                _ = self.model.generate(
                    **inputs,
                    max_new_tokens=1,
                    do_sample=False
                )
            
            return True
            
        except Exception as e:
            logger.error(f"Health check failed: {e}")
            return False
    
    def optimize_for_inference(self):
        """ì¶”ë¡  ìµœì í™”"""
        if not self.is_loaded:
            print("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        print("âš¡ ì¶”ë¡  ìµœì í™” ì ìš© ì¤‘...")
        
        try:
            # ëª¨ë¸ì„ evaluation ëª¨ë“œë¡œ ì„¤ì •
            self.model.eval()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
            for param in self.model.parameters():
                param.requires_grad = False
            
            # CUDA ìµœì í™” (ê°€ëŠ¥í•œ ê²½ìš°)
            if torch.cuda.is_available():
                torch.backends.cudnn.benchmark = True
                torch.backends.cudnn.deterministic = False
            
            print("âœ… ì¶”ë¡  ìµœì í™” ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âš ï¸ ìµœì í™” ì¤‘ ì˜¤ë¥˜: {e}")


# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_model_manager_instance = None

def get_model_manager(config: Optional[Config] = None) -> ModelManager:
    """ëª¨ë¸ ë§¤ë‹ˆì € ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _model_manager_instance
    
    if _model_manager_instance is None:
        if config is None:
            from config import config as default_config
            config = default_config
        _model_manager_instance = ModelManager(config)
    
    return _model_manager_instance


if __name__ == "__main__":
    # ëª¨ë¸ ë§¤ë‹ˆì € í…ŒìŠ¤íŠ¸
    from config import config
    
    manager = ModelManager(config)
    
    # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
    if manager.load_model():
        print("ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        
        # í—¬ìŠ¤ ì²´í¬
        if manager.health_check():
            print("âœ… ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
        else:
            print("âŒ ëª¨ë¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        info = manager.get_model_info()
        print(f"ğŸ“‹ ëª¨ë¸ ì •ë³´: {info}")
        
        # ìµœì í™” ì ìš©
        manager.optimize_for_inference()
        
        # ì–¸ë¡œë“œ
        manager.unload_model()
    else:
        print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
