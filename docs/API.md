# ğŸ“š Korean Bllossom AICA-5B API ì°¸ì¡°

ì´ ë¬¸ì„œëŠ” Korean Bllossom AICA-5B ì–‘ìí™” í”„ë¡œì íŠ¸ì˜ Python APIì— ëŒ€í•œ ìƒì„¸í•œ ì°¸ì¡° ìë£Œì…ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [í•µì‹¬ í´ë˜ìŠ¤](#í•µì‹¬-í´ë˜ìŠ¤)
- [ì„¤ì • ê´€ë¦¬](#ì„¤ì •-ê´€ë¦¬)
- [ëª¨ë¸ ê´€ë¦¬](#ëª¨ë¸-ê´€ë¦¬)
- [í…ìŠ¤íŠ¸ ìƒì„±](#í…ìŠ¤íŠ¸-ìƒì„±)
- [ì‹œê°-ì–¸ì–´ ìƒì„±](#ì‹œê°-ì–¸ì–´-ìƒì„±)
- [ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜](#ìœ í‹¸ë¦¬í‹°-í•¨ìˆ˜)
- [ì˜ˆì™¸ ì²˜ë¦¬](#ì˜ˆì™¸-ì²˜ë¦¬)

## ğŸ”§ í•µì‹¬ í´ë˜ìŠ¤

### Config

í”„ë¡œì íŠ¸ì˜ ì „ì²´ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from config import Config

# ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”
config = Config()

# YAML íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ
config = Config("custom_config.yaml")
```

#### ì†ì„±

| ì†ì„± | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `model` | `ModelConfig` | ëª¨ë¸ ê´€ë ¨ ì„¤ì • |
| `quantization` | `QuantizationConfig` | ì–‘ìí™” ì„¤ì • |
| `generation` | `GenerationConfig` | í…ìŠ¤íŠ¸ ìƒì„± ì„¤ì • |
| `hardware` | `HardwareConfig` | í•˜ë“œì›¨ì–´ ì„¤ì • |
| `paths` | `PathConfig` | ê²½ë¡œ ì„¤ì • |

#### ë©”ì„œë“œ

##### `load_from_yaml(config_file: str)`

YAML íŒŒì¼ì—ì„œ ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.

```python
config.load_from_yaml("custom_settings.yaml")
```

##### `save_to_yaml(config_file: str = None)`

í˜„ì¬ ì„¤ì •ì„ YAML íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

```python
config.save_to_yaml("saved_config.yaml")
```

##### `print_config()`

í˜„ì¬ ì„¤ì •ì„ ì½˜ì†”ì— ì¶œë ¥í•©ë‹ˆë‹¤.

```python
config.print_config()
```

### ModelManager

ëª¨ë¸ì˜ ë¡œë”©, ì–¸ë¡œë”©, ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from model_manager import ModelManager, get_model_manager

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸° (ê¶Œì¥)
manager = get_model_manager(config)

# ì§ì ‘ ìƒì„±
manager = ModelManager(config)
```

#### ì†ì„±

| ì†ì„± | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `model` | `MllamaForConditionalGeneration` | ë¡œë“œëœ ëª¨ë¸ |
| `processor` | `MllamaProcessor` | í† í¬ë‚˜ì´ì € ë° í”„ë¡œì„¸ì„œ |
| `device` | `str` | ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤ ("cuda" ë˜ëŠ” "cpu") |
| `is_loaded` | `bool` | ëª¨ë¸ ë¡œë“œ ìƒíƒœ |

#### ë©”ì„œë“œ

##### `load_model() -> bool`

ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.

```python
success = manager.load_model()
if success:
    print("ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
else:
    print("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨!")
```

**ë°˜í™˜ê°’**: ë¡œë“œ ì„±ê³µ ì—¬ë¶€ (`bool`)

##### `unload_model()`

ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì—ì„œ ì–¸ë¡œë“œí•©ë‹ˆë‹¤.

```python
manager.unload_model()
```

##### `clear_memory()`

GPU ë° ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.

```python
manager.clear_memory()
```

##### `get_model_info() -> dict`

í˜„ì¬ ëª¨ë¸ ìƒíƒœ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
info = manager.get_model_info()
print(f"ëª¨ë¸ ìƒíƒœ: {info['status']}")
print(f"VRAM ì‚¬ìš©ëŸ‰: {info.get('vram_allocated', 0):.1f}GB")
```

**ë°˜í™˜ê°’**: ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬

##### `health_check() -> bool`

ëª¨ë¸ì˜ ì •ìƒ ì‘ë™ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

```python
is_healthy = manager.health_check()
if is_healthy:
    print("ëª¨ë¸ì´ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
```

**ë°˜í™˜ê°’**: í—¬ìŠ¤ ì²´í¬ í†µê³¼ ì—¬ë¶€ (`bool`)

## ğŸ“ í…ìŠ¤íŠ¸ ìƒì„±

### TextGenerator

í…ìŠ¤íŠ¸ ìƒì„± ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from text_generator import TextGenerator

generator = TextGenerator(model_manager, config)
```

#### ë©”ì„œë“œ

##### `generate(prompt, **kwargs) -> dict`

ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
result = generator.generate(
    prompt="í•œêµ­ì˜ ì „í†µë¬¸í™”ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    max_tokens=300,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    repetition_penalty=1.1,
    do_sample=True,
    system_message="ë‹¹ì‹ ì€ í•œêµ­ ë¬¸í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
)
```

**ë§¤ê°œë³€ìˆ˜**:

| ë§¤ê°œë³€ìˆ˜ | íƒ€ì… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|----------|------|--------|------|
| `prompt` | `str` | í•„ìˆ˜ | ì…ë ¥ í”„ë¡¬í”„íŠ¸ |
| `max_tokens` | `int` | `config.generation.max_tokens` | ìµœëŒ€ ìƒì„± í† í° ìˆ˜ |
| `temperature` | `float` | `config.generation.temperature` | ìƒì„± ì˜¨ë„ (0.0-2.0) |
| `top_p` | `float` | `config.generation.top_p` | Nucleus sampling íŒŒë¼ë¯¸í„° |
| `top_k` | `int` | `config.generation.top_k` | Top-k sampling íŒŒë¼ë¯¸í„° |
| `repetition_penalty` | `float` | `config.generation.repetition_penalty` | ë°˜ë³µ ì–µì œ ì •ë„ |
| `do_sample` | `bool` | `config.generation.do_sample` | ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€ |
| `system_message` | `str` | `None` | ì‹œìŠ¤í…œ ë©”ì‹œì§€ |

**ë°˜í™˜ê°’**: ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

```python
{
    "success": True,
    "response": "ìƒì„±ëœ í…ìŠ¤íŠ¸ ë‚´ìš©",
    "full_response": "ì „ì²´ ì‘ë‹µ (ì…ë ¥ í¬í•¨)",
    "generation_time": 2.34,
    "input_tokens": 15,
    "output_tokens": 87,
    "tokens_per_second": 37.2,
    "parameters": {
        "max_tokens": 300,
        "temperature": 0.7,
        # ... ê¸°íƒ€ íŒŒë¼ë¯¸í„°
    }
}
```

##### `chat_generate(messages, **kwargs) -> dict`

ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
messages = [
    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
    {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"},
    {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"},
    {"role": "user", "content": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë–¤ê°€ìš”?"}
]

result = generator.chat_generate(messages, max_tokens=200)
```

**ë§¤ê°œë³€ìˆ˜**:
- `messages`: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸
- `**kwargs`: `generate()` ë©”ì„œë“œì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°

**ë°˜í™˜ê°’**: `generate()` ë©”ì„œë“œì™€ ë™ì¼í•œ êµ¬ì¡°

##### `batch_generate(prompts, **kwargs) -> List[dict]`

ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ì¼ê´„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
prompts = [
    "íŒŒì´ì¬ì˜ ì¥ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    "ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
    "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ ë¯¸ë˜ëŠ”?"
]

results = generator.batch_generate(prompts, max_tokens=200)

for i, result in enumerate(results):
    if result["success"]:
        print(f"ì§ˆë¬¸ {i+1}: {result['response']}")
```

**ë§¤ê°œë³€ìˆ˜**:
- `prompts`: í”„ë¡¬í”„íŠ¸ ë¦¬ìŠ¤íŠ¸
- `**kwargs`: ìƒì„± íŒŒë¼ë¯¸í„°

**ë°˜í™˜ê°’**: ìƒì„± ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸

### ConversationManager

ëŒ€í™” ê´€ë¦¬ ë° íˆìŠ¤í† ë¦¬ ì¶”ì  í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from text_generator import ConversationManager

conversation = ConversationManager(text_generator, max_history=10)
```

#### ì†ì„±

| ì†ì„± | íƒ€ì… | ì„¤ëª… |
|------|------|------|
| `conversation_history` | `List[dict]` | ëŒ€í™” íˆìŠ¤í† ë¦¬ |
| `system_message` | `str` | ì‹œìŠ¤í…œ ë©”ì‹œì§€ |
| `max_history` | `int` | ìµœëŒ€ íˆìŠ¤í† ë¦¬ ê¸¸ì´ |

#### ë©”ì„œë“œ

##### `set_system_message(message: str)`

ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```python
conversation.set_system_message("ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.")
```

##### `generate_response(user_input, **kwargs) -> dict`

ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

```python
response = conversation.generate_response(
    "ì•ˆë…•í•˜ì„¸ìš”!",
    max_tokens=200,
    temperature=0.7
)

if response["success"]:
    print(f"AI: {response['response']}")
```

##### `clear_history()`

ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

```python
conversation.clear_history()
```

##### `get_history_summary() -> dict`

ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½ì„ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
summary = conversation.get_history_summary()
print(f"ì´ ë©”ì‹œì§€ ìˆ˜: {summary['total_messages']}")
```

**ë°˜í™˜ê°’**:
```python
{
    "total_messages": 12,
    "user_messages": 6,
    "assistant_messages": 6,
    "system_message": "ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë‚´ìš©",
    "latest_messages": [...]  # ìµœê·¼ 4ê°œ ë©”ì‹œì§€
}
```

##### `export_conversation(filename: str = None) -> str`

ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.

```python
filename = conversation.export_conversation("my_chat.txt")
print(f"ëŒ€í™” ì €ì¥ë¨: {filename}")
```

**ë°˜í™˜ê°’**: ì €ì¥ëœ íŒŒì¼ëª…

## ğŸ–¼ï¸ ì‹œê°-ì–¸ì–´ ìƒì„±

### VisionGenerator

ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from vision_generator import VisionGenerator

vision_gen = VisionGenerator(model_manager, config)
```

#### ë©”ì„œë“œ

##### `generate_with_image(image_input, prompt, **kwargs) -> dict`

ì´ë¯¸ì§€ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
result = vision_gen.generate_with_image(
    image_input="photo.jpg",  # ë˜ëŠ” URL, PIL Image, bytes
    prompt="ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ë³´ì´ë‚˜ìš”?",
    max_tokens=250,
    temperature=0.1,
    preprocess=True
)
```

**ë§¤ê°œë³€ìˆ˜**:

| ë§¤ê°œë³€ìˆ˜ | íƒ€ì… | ì„¤ëª… |
|----------|------|------|
| `image_input` | `Union[str, Image.Image, bytes]` | ì´ë¯¸ì§€ ì…ë ¥ |
| `prompt` | `str` | í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ |
| `max_tokens` | `int` | ìµœëŒ€ ìƒì„± í† í° ìˆ˜ |
| `temperature` | `float` | ìƒì„± ì˜¨ë„ |
| `preprocess` | `bool` | ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì—¬ë¶€ |

**ì´ë¯¸ì§€ ì…ë ¥ í˜•ì‹**:
- ë¡œì»¬ íŒŒì¼ ê²½ë¡œ: `"./images/photo.jpg"`
- ì›¹ URL: `"https://example.com/image.png"`
- PIL Image ê°ì²´: `Image.open("photo.jpg")`
- ë°”ì´íŠ¸ ë°ì´í„°: `open("image.jpg", "rb").read()`
- Base64 ë°ì´í„° URL: `"data:image/jpeg;base64,/9j/4AAQ..."`

**ë°˜í™˜ê°’**:
```python
{
    "success": True,
    "response": "ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼",
    "full_response": "ì „ì²´ ì‘ë‹µ",
    "generation_time": 3.45,
    "input_tokens": 20,
    "output_tokens": 95,
    "tokens_per_second": 27.5,
    "image_size": (1024, 768),
    "parameters": {...}
}
```

##### `describe_image(image_input) -> dict`

ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤.

```python
result = vision_gen.describe_image("photo.jpg")
print(result['response'])
```

##### `extract_text(image_input) -> dict`

ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (OCR).

```python
result = vision_gen.extract_text("document.png")
print(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {result['response']}")
```

##### `analyze_chart(image_input) -> dict`

ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

```python
result = vision_gen.analyze_chart("sales_chart.png")
print(f"ì°¨íŠ¸ ë¶„ì„: {result['response']}")
```

##### `analyze_table(image_input) -> dict`

í‘œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

```python
result = vision_gen.analyze_table("data_table.jpg")
print(f"í‘œ ë¶„ì„: {result['response']}")
```

##### `convert_to_markdown(image_input) -> dict`

ë¬¸ì„œ ì´ë¯¸ì§€ë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

```python
result = vision_gen.convert_to_markdown("document.png")
with open("converted.md", "w") as f:
    f.write(result['response'])
```

##### `answer_visual_question(image_input, question) -> dict`

ì´ë¯¸ì§€ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤.

```python
result = vision_gen.answer_visual_question(
    "family_photo.jpg",
    "ì´ ì‚¬ì§„ì— ëª‡ ëª…ì˜ ì‚¬ëŒì´ ìˆë‚˜ìš”?"
)
print(f"ë‹µë³€: {result['response']}")
```

##### `batch_analyze_images(image_inputs, prompt, **kwargs) -> List[dict]`

ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ì¼ê´„ ë¶„ì„í•©ë‹ˆë‹¤.

```python
images = ["img1.jpg", "img2.jpg", "img3.jpg"]
results = vision_gen.batch_analyze_images(
    images,
    "ì´ ì´ë¯¸ì§€ë“¤ì˜ ê³µí†µì ì„ ì°¾ì•„ì£¼ì„¸ìš”"
)
```

### DocumentProcessor

ë¬¸ì„œ ì²˜ë¦¬ ì „ìš© í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

```python
from vision_generator import DocumentProcessor

doc_processor = DocumentProcessor(vision_generator)
```

#### ë©”ì„œë“œ

##### `process_document_page(image_input, task) -> dict`

ë‹¨ì¼ ë¬¸ì„œ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
result = doc_processor.process_document_page(
    "document_page.png",
    task="extract"  # "extract", "summarize", "markdown", "table"
)
```

**ì‘ì—… ìœ í˜•**:
- `"extract"`: í…ìŠ¤íŠ¸ ì¶”ì¶œ
- `"summarize"`: ë¬¸ì„œ ìš”ì•½
- `"markdown"`: ë§ˆí¬ë‹¤ìš´ ë³€í™˜
- `"table"`: í‘œ ë°ì´í„° ì¶”ì¶œ

##### `process_multi_page_document(image_inputs, task) -> dict`

ë‹¤ì¤‘ í˜ì´ì§€ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
pages = ["page1.png", "page2.png", "page3.png"]
result = doc_processor.process_multi_page_document(pages, "markdown")

print(f"ì²˜ë¦¬ëœ í˜ì´ì§€: {result['total_pages']}")
print(f"ì„±ê³µí•œ í˜ì´ì§€: {result['successful_pages']}")
print(f"ê²°í•©ëœ ë‚´ìš©:\n{result['combined_content']}")
```

**ë°˜í™˜ê°’**:
```python
{
    "success": True,
    "page_results": [...],  # ê° í˜ì´ì§€ ì²˜ë¦¬ ê²°ê³¼
    "combined_content": "ì „ì²´ ê²°í•©ëœ ë‚´ìš©",
    "total_pages": 3,
    "successful_pages": 3
}
```

## ğŸ› ï¸ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

### ì„¤ì • ê´€ë ¨

#### `setup_environment()`

í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```python
from config import setup_environment

setup_environment()
```

ì„¤ì •ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜:
- `HF_HOME`: Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬
- `TRANSFORMERS_CACHE`: Transformers ìºì‹œ ë””ë ‰í† ë¦¬
- `PYTORCH_CUDA_ALLOC_CONF`: CUDA ë©”ëª¨ë¦¬ ì„¤ì •
- `TOKENIZERS_PARALLELISM`: í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •

### ëª¨ë¸ ê´€ë¦¬

#### `get_model_manager(config) -> ModelManager`

ì‹±ê¸€í†¤ ëª¨ë¸ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

```python
from model_manager import get_model_manager

manager = get_model_manager(config)
```

## âš ï¸ ì˜ˆì™¸ ì²˜ë¦¬

### ì¼ë°˜ì ì¸ ì˜ˆì™¸

#### `torch.cuda.OutOfMemoryError`

VRAM ë¶€ì¡± ì‹œ ë°œìƒí•˜ëŠ” ì˜ˆì™¸ì…ë‹ˆë‹¤.

```python
try:
    result = generator.generate("ê¸´ í”„ë¡¬í”„íŠ¸...")
except torch.cuda.OutOfMemoryError:
    print("VRAM ë¶€ì¡±! ë©”ëª¨ë¦¬ë¥¼ ì •ë¦¬í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
    manager.clear_memory()
```

#### `ValueError`

ì˜ëª»ëœ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš© ì‹œ ë°œìƒí•©ë‹ˆë‹¤.

```python
try:
    result = generator.generate("")  # ë¹ˆ í”„ë¡¬í”„íŠ¸
except ValueError as e:
    print(f"ë§¤ê°œë³€ìˆ˜ ì˜¤ë¥˜: {e}")
```

#### `ConnectionError`

ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ì˜¤ë¥˜ì…ë‹ˆë‹¤.

```python
try:
    result = vision_gen.describe_image("https://broken-url.com/image.jpg")
except requests.ConnectionError:
    print("ì´ë¯¸ì§€ URLì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
```

### ì•ˆì „í•œ í˜¸ì¶œ íŒ¨í„´

#### ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

```python
import time

def safe_generate(generator, prompt, max_retries=3):
    """ì•ˆì „í•œ ìƒì„± í•¨ìˆ˜ (ì¬ì‹œë„ í¬í•¨)"""
    
    for attempt in range(max_retries):
        try:
            result = generator.generate(prompt)
            if result["success"]:
                return result
            else:
                print(f"ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {result['error']}")
                
        except Exception as e:
            print(f"ì‹œë„ {attempt + 1} ì˜ˆì™¸: {e}")
            
        # ì¬ì‹œë„ ì „ ëŒ€ê¸° ë° ë©”ëª¨ë¦¬ ì •ë¦¬
        time.sleep(2)
        generator.model_manager.clear_memory()
    
    return {"success": False, "error": "ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨"}

# ì‚¬ìš© ì˜ˆì‹œ
result = safe_generate(generator, "ë³µì¡í•œ ì§ˆë¬¸")
```

#### ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €

```python
from contextlib import contextmanager

@contextmanager
def model_context(config):
    """ëª¨ë¸ ìë™ ê´€ë¦¬ ì»¨í…ìŠ¤íŠ¸"""
    manager = get_model_manager(config)
    
    try:
        if not manager.is_loaded:
            success = manager.load_model()
            if not success:
                raise RuntimeError("ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        
        yield manager
        
    finally:
        manager.clear_memory()

# ì‚¬ìš© ì˜ˆì‹œ
with model_context(config) as manager:
    generator = TextGenerator(manager, config)
    result = generator.generate("í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸")
    print(result['response'])
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì»¤ìŠ¤í…€ ì„¤ì • í´ë˜ìŠ¤

```python
from config import Config
from dataclasses import dataclass

@dataclass
class CustomConfig(Config):
    """ì‚¬ìš©ì ì •ì˜ ì„¤ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        super().__init__()
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.generation.max_tokens = 200
        self.generation.temperature = 0.5
        self.hardware.max_memory_usage = 0.8
    
    def set_creative_mode(self):
        """ì°½ì˜ì  ëª¨ë“œ ì„¤ì •"""
        self.generation.temperature = 0.9
        self.generation.top_p = 0.95
        self.generation.do_sample = True
    
    def set_precise_mode(self):
        """ì •í™•í•œ ëª¨ë“œ ì„¤ì •"""
        self.generation.temperature = 0.1
        self.generation.do_sample = False

# ì‚¬ìš© ì˜ˆì‹œ
custom_config = CustomConfig()
custom_config.set_creative_mode()
```

### ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°

```python
import time
import functools

def monitor_performance(func):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        try:
            result = func(*args, **kwargs)
            
            # ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
            if isinstance(result, dict) and "success" in result:
                end_time = time.time()
                end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
                
                result["performance"] = {
                    "execution_time": end_time - start_time,
                    "memory_used": (end_memory - start_memory) / 1024**3,
                    "function_name": func.__name__
                }
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            print(f"í•¨ìˆ˜ {func.__name__} ì‹¤í–‰ ì‹¤íŒ¨ ({execution_time:.2f}ì´ˆ): {e}")
            raise
    
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@monitor_performance
def generate_with_monitoring(generator, prompt):
    return generator.generate(prompt)

result = generate_with_monitoring(generator, "í…ŒìŠ¤íŠ¸")
print(f"ì‹¤í–‰ ì‹œê°„: {result['performance']['execution_time']:.2f}ì´ˆ")
```

### ë¹„ë™ê¸° ì²˜ë¦¬

```python
import asyncio
import concurrent.futures

async def async_batch_generate(generator, prompts, max_workers=2):
    """ë¹„ë™ê¸° ë°°ì¹˜ ìƒì„±"""
    
    def generate_single(prompt):
        return generator.generate(prompt)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(executor, generate_single, prompt)
            for prompt in prompts
        ]
        
        results = await asyncio.gather(*tasks)
        return results

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    prompts = ["ì§ˆë¬¸ 1", "ì§ˆë¬¸ 2", "ì§ˆë¬¸ 3"]
    results = await async_batch_generate(generator, prompts)
    
    for i, result in enumerate(results):
        print(f"ê²°ê³¼ {i+1}: {result['response']}")

# asyncio.run(main())
```

---

ì´ API ì°¸ì¡° ë¬¸ì„œë¥¼ í†µí•´ Korean Bllossom AICA-5B í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ì˜ˆì œê°€ í•„ìš”í•˜ì‹œë©´ GitHub Issuesë¥¼ í†µí•´ ë¬¸ì˜í•´ì£¼ì„¸ìš”.