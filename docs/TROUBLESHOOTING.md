# ğŸ”§ Korean Bllossom AICA-5B ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

ì´ ë¬¸ì„œëŠ” Korean Bllossom AICA-5B ì–‘ìí™” í”„ë¡œì íŠ¸ ì‚¬ìš© ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ì„¤ì¹˜ ê´€ë ¨ ë¬¸ì œ](#ì„¤ì¹˜-ê´€ë ¨-ë¬¸ì œ)
- [ëª¨ë¸ ë¡œë”© ë¬¸ì œ](#ëª¨ë¸-ë¡œë”©-ë¬¸ì œ)
- [ë©”ëª¨ë¦¬ ê´€ë ¨ ë¬¸ì œ](#ë©”ëª¨ë¦¬-ê´€ë ¨-ë¬¸ì œ)
- [ì„±ëŠ¥ ê´€ë ¨ ë¬¸ì œ](#ì„±ëŠ¥-ê´€ë ¨-ë¬¸ì œ)
- [ë„¤íŠ¸ì›Œí¬ ê´€ë ¨ ë¬¸ì œ](#ë„¤íŠ¸ì›Œí¬-ê´€ë ¨-ë¬¸ì œ)
- [í•˜ë“œì›¨ì–´ ê´€ë ¨ ë¬¸ì œ](#í•˜ë“œì›¨ì–´-ê´€ë ¨-ë¬¸ì œ)
- [ìì£¼ ë¬»ëŠ” ì§ˆë¬¸](#ìì£¼-ë¬»ëŠ”-ì§ˆë¬¸)
- [ì§„ë‹¨ ë„êµ¬](#ì§„ë‹¨-ë„êµ¬)

## ğŸ› ï¸ ì„¤ì¹˜ ê´€ë ¨ ë¬¸ì œ

### 1. CUDA ê´€ë ¨ ì˜¤ë¥˜

#### ë¬¸ì œ: `RuntimeError: CUDA runtime error`

**ì¦ìƒ**:
```
RuntimeError: CUDA runtime error (2) : out of memory
```

**ì›ì¸**: 
- CUDA ë“œë¼ì´ë²„ì™€ PyTorch ë²„ì „ ë¶ˆì¼ì¹˜
- êµ¬ë²„ì „ CUDA ì‚¬ìš©

**í•´ê²°ì±…**:

```bash
# 1. CUDA ë²„ì „ í™•ì¸
nvidia-smi
nvcc --version

# 2. PyTorch ì¬ì„¤ì¹˜ (CUDA 12.1 ê¸°ì¤€)
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. CUDA 11.8ì¸ ê²½ìš°
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# 4. ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

#### ë¬¸ì œ: `NVIDIA-SMI has failed`

**ì¦ìƒ**:
```bash
$ nvidia-smi
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver.
```

**í•´ê²°ì±…**:

```bash
# 1. ë“œë¼ì´ë²„ ìƒíƒœ í™•ì¸
lsmod | grep nvidia

# 2. ìë™ ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo ubuntu-drivers autoinstall

# 3. ìˆ˜ë™ ë“œë¼ì´ë²„ ì„¤ì¹˜
sudo apt update
sudo apt install nvidia-driver-525  # ë˜ëŠ” ìµœì‹  ë²„ì „

# 4. ì‹œìŠ¤í…œ ì¬ë¶€íŒ…
sudo reboot

# 5. í™•ì¸
nvidia-smi
```

### 2. Python íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì˜¤ë¥˜

#### ë¬¸ì œ: `bitsandbytes` ì„¤ì¹˜ ì‹¤íŒ¨

**ì¦ìƒ**:
```
ERROR: Failed building wheel for bitsandbytes
```

**í•´ê²°ì±…**:

```bash
# ë°©ë²• 1: ì‚¬ì „ ì»´íŒŒì¼ëœ ë²„ì „ ì„¤ì¹˜
pip install bitsandbytes --no-cache-dir

# ë°©ë²• 2: CUDA ê²½ë¡œ ì„¤ì • í›„ ì„¤ì¹˜
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
pip install bitsandbytes --no-binary bitsandbytes

# ë°©ë²• 3: conda ì‚¬ìš©
conda install -c conda-forge bitsandbytes

# ë°©ë²• 4: íŠ¹ì • ë²„ì „ ì„¤ì¹˜
pip install bitsandbytes==0.41.0
```

#### ë¬¸ì œ: `transformers` ë²„ì „ ì¶©ëŒ

**ì¦ìƒ**:
```
AttributeError: module 'transformers' has no attribute 'MllamaForConditionalGeneration'
```

**í•´ê²°ì±…**:

```bash
# 1. ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ê·¸ë ˆì´ë“œ
pip install transformers --upgrade

# 2. íŠ¹ì • ë²„ì „ ê°•ì œ ì„¤ì¹˜
pip install transformers==4.40.0 --force-reinstall

# 3. ê°œë°œ ë²„ì „ ì„¤ì¹˜ (ìµœì‹  ê¸°ëŠ¥ í•„ìš”ì‹œ)
pip install git+https://github.com/huggingface/transformers.git

# 4. ì„¤ì¹˜ í™•ì¸
python -c "from transformers import MllamaForConditionalGeneration; print('OK')"
```

### 3. ê°€ìƒí™˜ê²½ ë¬¸ì œ

#### ë¬¸ì œ: ê°€ìƒí™˜ê²½ì—ì„œ íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

**í•´ê²°ì±…**:

```bash
# 1. ê°€ìƒí™˜ê²½ ì¬ìƒì„±
rm -rf bllossom_env
python3 -m venv bllossom_env
source bllossom_env/bin/activate

# 2. pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip setuptools wheel

# 3. íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip install -r requirements.txt

# 4. í™˜ê²½ í™•ì¸
which python
which pip
```

## ğŸ¤– ëª¨ë¸ ë¡œë”© ë¬¸ì œ

### 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨

#### ë¬¸ì œ: `ConnectionError` ë˜ëŠ” ë‹¤ìš´ë¡œë“œ ì¤‘ë‹¨

**í•´ê²°ì±…**:

```bash
# 1. ë„¤íŠ¸ì›Œí¬ í™•ì¸
ping huggingface.co

# 2. ìºì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
rm -rf ~/.cache/huggingface/
rm -rf ./model_cache/

# 3. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python -c "
from transformers import MllamaProcessor
processor = MllamaProcessor.from_pretrained(
    'Bllossom/llama-3.2-Korean-Bllossom-AICA-5B',
    cache_dir='./model_cache',
    resume_download=True
)
print('ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!')
"

# 4. í”„ë¡ì‹œ ì„¤ì • (íšŒì‚¬ ë„¤íŠ¸ì›Œí¬ì¸ ê²½ìš°)
export https_proxy=http://proxy.company.com:8080
export http_proxy=http://proxy.company.com:8080
```

#### ë¬¸ì œ: `OSError: Can't load tokenizer`

**í•´ê²°ì±…**:

```bash
# 1. í† í¬ë‚˜ì´ì €ë§Œ ë³„ë„ ë‹¤ìš´ë¡œë“œ
python -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(
    'Bllossom/llama-3.2-Korean-Bllossom-AICA-5B',
    trust_remote_code=True
)
"

# 2. ìºì‹œ ê¶Œí•œ í™•ì¸
sudo chown -R $USER ~/.cache/huggingface/
chmod -R 755 ~/.cache/huggingface/

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export HF_HOME=./model_cache
export TRANSFORMERS_CACHE=./model_cache
```

### 2. ì–‘ìí™” ë¡œë”© ì˜¤ë¥˜

#### ë¬¸ì œ: `ImportError: bitsandbytes`

**ì¦ìƒ**:
```
ImportError: bitsandbytes is not installed. Please install it with `pip install bitsandbytes`.
```

**í•´ê²°ì±…**:

```python
# config.yamlì—ì„œ ì–‘ìí™” ë¹„í™œì„±í™”
quantization:
  load_in_4bit: false

# ë˜ëŠ” Python ì½”ë“œì—ì„œ
config.quantization.load_in_4bit = False
```

```bash
# bitsandbytes ì¬ì„¤ì¹˜
pip uninstall bitsandbytes
pip install bitsandbytes --no-cache-dir
```

## ğŸ’¾ ë©”ëª¨ë¦¬ ê´€ë ¨ ë¬¸ì œ

### 1. VRAM ë¶€ì¡± (OOM) ì˜¤ë¥˜

#### ë¬¸ì œ: `torch.cuda.OutOfMemoryError`

**ì¦ìƒ**:
```
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB
```

**í•´ê²°ì±… (ìš°ì„ ìˆœìœ„ëŒ€ë¡œ)**:

```bash
# 1. ë‹¤ë¥¸ GPU í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
nvidia-smi
sudo kill -9 <PID>

# 2. ì‹œìŠ¤í…œ ì¬ë¶€íŒ…
sudo reboot
```

```python
# 3. ë©”ëª¨ë¦¬ ì„¤ì • ì¡°ì •
config.generation.max_tokens = 128  # ê¸°ë³¸ê°’ 256ì—ì„œ ì¤„ì„
config.hardware.max_memory_usage = 0.7  # 70%ë§Œ ì‚¬ìš©

# 4. ì–‘ìí™” ê°•í™”
config.quantization.load_in_4bit = True
config.quantization.bnb_4bit_compute_dtype = "float16"

# 5. ìºì‹œ ë¹„í™œì„±í™”
use_cache = False

# 6. ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
# í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ì²˜ë¦¬
```

```bash
# 7. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

#### ë¬¸ì œ: ì‹œìŠ¤í…œ RAM ë¶€ì¡±

**í•´ê²°ì±…**:

```bash
# 1. ìŠ¤ì™‘ ë©”ëª¨ë¦¬ ìƒì„±
sudo fallocate -l 8G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# 2. ì˜êµ¬ ì„¤ì •
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab

# 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
free -h
```

### 2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜

#### ë¬¸ì œ: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê³„ì† ì¦ê°€

**í•´ê²°ì±…**:

```python
# 1. ëª…ì‹œì  ë©”ëª¨ë¦¬ ì •ë¦¬
import gc
import torch

def clear_memory():
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

# 2. ì‘ì—… í›„ í•­ìƒ ë©”ëª¨ë¦¬ ì •ë¦¬
result = generator.generate("prompt")
clear_memory()

#